{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier():\n",
    "    def __init__ (self,model_arch,no_examples):\n",
    "        self.in_size = model_arch[0]\n",
    "        self.out_size = model_arch[-1]\n",
    "        DeepClassifier.m = no_examples\n",
    "        self.hidden = model_arch[1:-1]\n",
    "        param = {}\n",
    "        DeepClassifier.model_arch = model_arch\n",
    "        \n",
    "        for l in range(1, len(model_arch)):\n",
    "            param['W' + str(l)] = np.random.randn(DeepClassifier.model_arch[l], DeepClassifier.model_arch[l-1]) \n",
    "            param['b' + str(l)] = np.zeros((DeepClassifier.model_arch[l], 1))\n",
    "        DeepClassifier.param = param\n",
    "        DeepClassifier.losslist = []\n",
    "\n",
    "    def linear_forward(A,w,b):\n",
    "        z = np.dot(w,A) + b\n",
    "        cache = (A,w,b)\n",
    "        return z,cache\n",
    "    \n",
    "    def sigmoid(z):\n",
    "        A = 1/(1+np.exp(-z))\n",
    "        cache = z\n",
    "        return A,cache\n",
    "    \n",
    "    def relu(z):\n",
    "        A = np.maximum(0,z)\n",
    "        cache = z\n",
    "        return A,cache\n",
    "        \n",
    "    def linear_activation_forward(A_prev,w,b,activation):\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            Z,linear_cache  = DeepClassifier.linear_forward(A_prev,w,b)\n",
    "            A,activation_cache  = DeepClassifier.sigmoid(Z)\n",
    "            \n",
    "        elif activation == \"relu\":\n",
    "                Z, linear_cache = DeepClassifier.linear_forward(A_prev,w,b)\n",
    "                A, activation_cache = DeepClassifier.relu(Z)\n",
    "\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        return A, cache\n",
    "    \n",
    "    def L_model_forward(x,param):\n",
    "        caches = []\n",
    "        A = x\n",
    "        \n",
    "        for l in range(1,len(DeepClassifier.model_arch)-1):\n",
    "            A_prev = A\n",
    "            A, cache = DeepClassifier.linear_activation_forward(A_prev,DeepClassifier.param['W' + str(l)],DeepClassifier.param['b' + str(l)], activation = \"relu\")            \n",
    "            caches.append(cache)\n",
    "   \n",
    "        AL, cache = DeepClassifier.linear_activation_forward(A, DeepClassifier.param['W' + str(len(DeepClassifier.model_arch)-1)], DeepClassifier.param['b' + str(len(DeepClassifier.model_arch)-1)], activation = \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "\n",
    "        return AL, caches\n",
    "\n",
    "    def compute_cost(AL, Y):\n",
    "        \n",
    "        cost = -1./DeepClassifier.m * np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "        cost = np.squeeze(cost)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def linear_backward(dZ, cache):\n",
    "        A_prev, W, b = cache\n",
    "        \n",
    "        dW = 1./DeepClassifier.m * np.dot(dZ, A_prev.T)\n",
    "        db = 1./DeepClassifier.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    "    def sigmoid_backward(dA, cache):\n",
    "   \n",
    "        Z = cache \n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA * s * (1-s)\n",
    "        \n",
    "        return dZ\n",
    "\n",
    "    def relu_backward(dA, cache):\n",
    "    \n",
    "        Z = cache\n",
    "        dZ = np.array(dA, copy=True)  \n",
    "        dZ[Z <= 0] = 0\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def linear_activation_backward(dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "    \n",
    "        if activation == \"relu\":\n",
    "            dZ = DeepClassifier.relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = DeepClassifier.linear_backward(dZ, linear_cache)\n",
    "\n",
    "        elif activation == \"sigmoid\":\n",
    "            dZ = DeepClassifier.sigmoid_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = DeepClassifier.linear_backward(dZ, linear_cache)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def L_model_backward(AL, Y, caches):\n",
    "        grads = {}\n",
    "        \n",
    "        L = len(DeepClassifier.model_arch)-1\n",
    "        \n",
    "        \n",
    "        Y = Y.reshape(AL.shape)\n",
    "\n",
    "        \n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "        current_cache = caches[L-1]\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = DeepClassifier.linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "       \n",
    "        for l in reversed(range(L-1)):\n",
    "            \n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = DeepClassifier.linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, \"relu\")\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def update_parameters(param, grads, learning_rate):\n",
    "    \n",
    "        L = len(DeepClassifier.model_arch)-1 \n",
    "        \n",
    "        for l in range(L):\n",
    "            DeepClassifier.param[\"W\" + str(l+1)] = DeepClassifier.param[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "            DeepClassifier.param[\"b\" + str(l+1)] = DeepClassifier.param[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        return param\n",
    "\n",
    "\n",
    "    def fit(self,x_train,y_train,epochs,learning_rate):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "                AL,caches = DeepClassifier.L_model_forward(x_train,DeepClassifier.param)\n",
    "                cost = DeepClassifier.compute_cost(AL, y_train)\n",
    "                self.losslist.append(cost)\n",
    "                if epoch % 1000 == 0:\n",
    "                    print(f\"loss is {cost} in {epoch}th epoch \")\n",
    "                grads = DeepClassifier.L_model_backward(AL, y_train, caches)\n",
    "                self.param = DeepClassifier.update_parameters(DeepClassifier.param, grads, learning_rate)\n",
    "                    \n",
    "\n",
    "    def Predict(self,x):\n",
    "        a = x\n",
    "        for i in range(1,len(self.model_arch)-1):       \n",
    "            z = np.dot(self.param[\"w\" + str(i)].T,a) + self.param[\"b\" + str(i)]\n",
    "            a = np.tanh(z)\n",
    "            \n",
    "        z = np.dot(self.param[\"w\" + str(len(self.model_arch)-1)].T,a) + self.param[\"b\" + str(len(self.model_arch)-1)]\n",
    "        a = np.tanh(z)\n",
    "        return a  \n",
    "                      \n",
    "    def predict(self,x):\n",
    "        A = x\n",
    "        for l in range(1,len(DeepClassifier.model_arch)-1):\n",
    "            A_prev = A\n",
    "            A, cache = DeepClassifier.linear_activation_forward(A_prev,DeepClassifier.param['W' + str(l)],DeepClassifier.param['b' + str(l)], activation = \"relu\")            \n",
    "            \n",
    "        AL, cache = DeepClassifier.linear_activation_forward(A, DeepClassifier.param['W' + str(len(DeepClassifier.model_arch)-1)], DeepClassifier.param['b' + str(len(DeepClassifier.model_arch)-1)], activation = \"sigmoid\")\n",
    "        \n",
    "\n",
    "        return AL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"P:/tech/datasets/titanic_prep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop([\"Survived\",\"Unnamed: 0\"],axis = 1).values\n",
    "y = data[\"Survived\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sc = scaler.fit_transform(x_train)\n",
    "x_test_sc = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = DeepClassifier(model_arch=[8,7,5,1],no_examples=x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 1.6073184382972237 in 0th epoch \n",
      "loss is 0.38798293432390624 in 1000th epoch \n",
      "loss is 0.3776846125686094 in 2000th epoch \n",
      "loss is 0.36974276267612016 in 3000th epoch \n",
      "loss is 0.3652945108664856 in 4000th epoch \n",
      "loss is 0.36284433694535817 in 5000th epoch \n",
      "loss is 0.3612376337576664 in 6000th epoch \n",
      "loss is 0.35838988404912114 in 7000th epoch \n",
      "loss is 0.35728128767982653 in 8000th epoch \n",
      "loss is 0.3549743598062577 in 9000th epoch \n",
      "loss is 0.3508297350749894 in 10000th epoch \n",
      "loss is 0.35069770973866393 in 11000th epoch \n",
      "loss is 0.3496580203492877 in 12000th epoch \n",
      "loss is 0.3495383446374441 in 13000th epoch \n",
      "loss is 0.34872960718301416 in 14000th epoch \n",
      "loss is 0.3476928728893747 in 15000th epoch \n",
      "loss is 0.3478223502084553 in 16000th epoch \n",
      "loss is 0.34861849428875646 in 17000th epoch \n",
      "loss is 0.3476448519264106 in 18000th epoch \n",
      "loss is 0.3473325737664805 in 19000th epoch \n",
      "loss is 0.3489632158424165 in 20000th epoch \n",
      "loss is 0.3495025346291261 in 21000th epoch \n",
      "loss is 0.3492737077442277 in 22000th epoch \n",
      "loss is 0.3492872497785105 in 23000th epoch \n",
      "loss is 0.34941029152416475 in 24000th epoch \n",
      "loss is 0.3486738911768263 in 25000th epoch \n",
      "loss is 0.3487535912088872 in 26000th epoch \n",
      "loss is 0.34948895052364154 in 27000th epoch \n",
      "loss is 0.34824168740374173 in 28000th epoch \n",
      "loss is 0.3485169253209389 in 29000th epoch \n",
      "loss is 0.34824257703904055 in 30000th epoch \n",
      "loss is 0.34999732684632096 in 31000th epoch \n",
      "loss is 0.3491897376677808 in 32000th epoch \n",
      "loss is 0.34898769048477124 in 33000th epoch \n",
      "loss is 0.34892206812995796 in 34000th epoch \n",
      "loss is 0.35035713606169067 in 35000th epoch \n",
      "loss is 0.3508234282060797 in 36000th epoch \n",
      "loss is 0.34955102429553964 in 37000th epoch \n",
      "loss is 0.34865978098265304 in 38000th epoch \n",
      "loss is 0.34931583863304 in 39000th epoch \n",
      "loss is 0.3494223595711076 in 40000th epoch \n",
      "loss is 0.3508319130293628 in 41000th epoch \n",
      "loss is 0.3507566225392567 in 42000th epoch \n",
      "loss is 0.3500750195261625 in 43000th epoch \n",
      "loss is 0.350521110568415 in 44000th epoch \n",
      "loss is 0.35189908731407266 in 45000th epoch \n",
      "loss is 0.35006837025259674 in 46000th epoch \n",
      "loss is 0.3506943066617916 in 47000th epoch \n",
      "loss is 0.35232584735881173 in 48000th epoch \n",
      "loss is 0.3497228999599928 in 49000th epoch \n",
      "loss is 0.349530060048562 in 50000th epoch \n",
      "loss is 0.3490440564391994 in 51000th epoch \n",
      "loss is 0.35118992140230415 in 52000th epoch \n",
      "loss is 0.3490250826848623 in 53000th epoch \n",
      "loss is 0.3494154079093976 in 54000th epoch \n",
      "loss is 0.3533138267245666 in 55000th epoch \n",
      "loss is 0.34616481821526507 in 56000th epoch \n",
      "loss is 0.3602309103411607 in 57000th epoch \n",
      "loss is 0.34692337979032284 in 58000th epoch \n",
      "loss is 0.349458693316227 in 59000th epoch \n"
     ]
    }
   ],
   "source": [
    "cls.fit(x_train_sc.T,y_train.reshape(1,-1),epochs = 60000,learning_rate= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cls.predict(x_test_sc.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[122  19]\n",
      " [ 21  61]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.820627802690583"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(y_pred.reshape(-1),y_test))\n",
    "accuracy_score(y_test,y_pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
