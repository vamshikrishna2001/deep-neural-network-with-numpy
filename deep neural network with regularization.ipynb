{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier():\n",
    "    def __init__ (self,model_arch,no_examples,lambd):\n",
    "        self.in_size = model_arch[0]\n",
    "        self.out_size = model_arch[-1]\n",
    "        DeepClassifier.m = no_examples\n",
    "        self.hidden = model_arch[1:-1]\n",
    "        param = {}\n",
    "        DeepClassifier.model_arch = model_arch\n",
    "        DeepClassifier.lambd = lambd\n",
    "        \n",
    "        for l in range(1, len(model_arch)):\n",
    "            param['W' + str(l)] = np.random.randn(DeepClassifier.model_arch[l], DeepClassifier.model_arch[l-1])*np.sqrt(2 / DeepClassifier.model_arch[l - 1]) \n",
    "            param['b' + str(l)] = np.zeros((DeepClassifier.model_arch[l], 1))\n",
    "        DeepClassifier.param = param\n",
    "        DeepClassifier.losslist = []\n",
    "\n",
    "    def linear_forward(A,w,b):\n",
    "        z = np.dot(w,A) + b\n",
    "        cache = (A,w,b)\n",
    "        return z,cache\n",
    "    \n",
    "    def sigmoid(z):\n",
    "        A = 1/(1+np.exp(-z))\n",
    "        cache = z\n",
    "        return A,cache\n",
    "    \n",
    "    def relu(z):\n",
    "        A = np.maximum(0,z)\n",
    "        cache = z\n",
    "        return A,cache\n",
    "        \n",
    "    def linear_activation_forward(A_prev,w,b,activation):\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            Z,linear_cache  = DeepClassifier.linear_forward(A_prev,w,b)\n",
    "            A,activation_cache  = DeepClassifier.sigmoid(Z)\n",
    "            \n",
    "        elif activation == \"relu\":\n",
    "                Z, linear_cache = DeepClassifier.linear_forward(A_prev,w,b)\n",
    "                A, activation_cache = DeepClassifier.relu(Z)\n",
    "\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        return A, cache\n",
    "    \n",
    "    def L_model_forward(x,param):\n",
    "        caches = []\n",
    "        A = x\n",
    "        \n",
    "        for l in range(1,len(DeepClassifier.model_arch)-1):\n",
    "            A_prev = A\n",
    "            A, cache = DeepClassifier.linear_activation_forward(A_prev,DeepClassifier.param['W' + str(l)],DeepClassifier.param['b' + str(l)], activation = \"relu\")            \n",
    "            caches.append(cache)\n",
    "   \n",
    "        AL, cache = DeepClassifier.linear_activation_forward(A, DeepClassifier.param['W' + str(len(DeepClassifier.model_arch)-1)], DeepClassifier.param['b' + str(len(DeepClassifier.model_arch)-1)], activation = \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "\n",
    "        return AL, caches\n",
    "\n",
    "    def compute_cost(AL, Y):\n",
    "        \n",
    "        cost = -1./DeepClassifier.m * np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "        cost = np.squeeze(cost)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def linear_backward(dZ, cache):\n",
    "        A_prev, W, b = cache\n",
    "        \n",
    "        dW = 1./DeepClassifier.m * np.dot(dZ, A_prev.T)\n",
    "        db = 1./DeepClassifier.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    "    def sigmoid_backward(dA, cache):\n",
    "   \n",
    "        Z = cache \n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA * s * (1-s)\n",
    "        \n",
    "        return dZ\n",
    "\n",
    "    def relu_backward(dA, cache):\n",
    "    \n",
    "        Z = cache\n",
    "        dZ = np.array(dA, copy=True)  \n",
    "        dZ[Z <= 0] = 0\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def linear_activation_backward(dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "    \n",
    "        if activation == \"relu\":\n",
    "            dZ = DeepClassifier.relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = DeepClassifier.linear_backward(dZ, linear_cache)\n",
    "\n",
    "        elif activation == \"sigmoid\":\n",
    "            dZ = DeepClassifier.sigmoid_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = DeepClassifier.linear_backward(dZ, linear_cache)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def L_model_backward(AL, Y, caches):\n",
    "        grads = {}\n",
    "        \n",
    "        L = len(DeepClassifier.model_arch)-1\n",
    "        \n",
    "        \n",
    "        Y = Y.reshape(AL.shape)\n",
    "\n",
    "        \n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "        current_cache = caches[L-1]\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = DeepClassifier.linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "       \n",
    "        for l in reversed(range(L-1)):\n",
    "            \n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = DeepClassifier.linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, \"relu\")\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp + (DeepClassifier.lambd *DeepClassifier.param[\"W\" + str(l+1)] ) /DeepClassifier.m\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def update_parameters(param, grads, learning_rate):\n",
    "    \n",
    "        L = len(DeepClassifier.model_arch)-1 \n",
    "        \n",
    "        for l in range(L):\n",
    "            DeepClassifier.param[\"W\" + str(l+1)] = DeepClassifier.param[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "            DeepClassifier.param[\"b\" + str(l+1)] = DeepClassifier.param[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        return param\n",
    "\n",
    "\n",
    "    def fit(self,x_train,y_train,epochs,learning_rate):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "                AL,caches = DeepClassifier.L_model_forward(x_train,DeepClassifier.param)\n",
    "                cost = DeepClassifier.compute_cost(AL, y_train)\n",
    "                self.losslist.append(cost)\n",
    "                if epoch % 1000 == 0:\n",
    "                    print(f\"loss is {cost} in {epoch}th epoch \")\n",
    "                grads = DeepClassifier.L_model_backward(AL, y_train, caches)\n",
    "                self.param = DeepClassifier.update_parameters(DeepClassifier.param, grads, learning_rate)\n",
    "                    \n",
    "\n",
    "    def predict(self,x):\n",
    "        A = x\n",
    "        for l in range(1,len(DeepClassifier.model_arch)-1):\n",
    "            A_prev = A\n",
    "            A, cache = DeepClassifier.linear_activation_forward(A_prev,DeepClassifier.param['W' + str(l)],DeepClassifier.param['b' + str(l)], activation = \"relu\")            \n",
    "            \n",
    "        AL, cache = DeepClassifier.linear_activation_forward(A, DeepClassifier.param['W' + str(len(DeepClassifier.model_arch)-1)], DeepClassifier.param['b' + str(len(DeepClassifier.model_arch)-1)], activation = \"sigmoid\")\n",
    "        \n",
    "\n",
    "        return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"P:/tech/datasets/titanic_prep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop([\"Survived\",\"Unnamed: 0\"],axis = 1).values\n",
    "y = data[\"Survived\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sc = scaler.fit_transform(x_train)\n",
    "x_test_sc = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = DeepClassifier(model_arch=[8,7,5,3,1],no_examples=x_train.shape[0],lambd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 1.2914731080173714 in 0th epoch \n",
      "loss is 0.40672602678488257 in 1000th epoch \n",
      "loss is 0.40069411086836404 in 2000th epoch \n",
      "loss is 0.3962454599339615 in 3000th epoch \n",
      "loss is 0.39120194283032017 in 4000th epoch \n",
      "loss is 0.3890115328743963 in 5000th epoch \n",
      "loss is 0.3866333111590877 in 6000th epoch \n",
      "loss is 0.38333678413398586 in 7000th epoch \n",
      "loss is 0.374807455806574 in 8000th epoch \n",
      "loss is 0.35602627773291756 in 9000th epoch \n",
      "loss is 0.36390333248301804 in 10000th epoch \n",
      "loss is 0.3658748778942003 in 11000th epoch \n",
      "loss is 0.3549280202307234 in 12000th epoch \n",
      "loss is 0.36173379337231526 in 13000th epoch \n",
      "loss is 0.36178408009682894 in 14000th epoch \n",
      "loss is 0.3596619775714164 in 15000th epoch \n",
      "loss is 0.35336057865285997 in 16000th epoch \n",
      "loss is 0.3764724659733736 in 17000th epoch \n",
      "loss is 0.3677497004330544 in 18000th epoch \n",
      "loss is 0.3541436502253335 in 19000th epoch \n"
     ]
    }
   ],
   "source": [
    "cls.fit(x_train_sc.T,y_train.reshape(1,-1),epochs = 20000,learning_rate= 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cls.predict(x_test_sc.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130  21]\n",
      " [ 11  61]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8565022421524664"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(y_pred.reshape(-1),y_test))\n",
    "accuracy_score(y_test,y_pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
